{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/06/28 13:17:13] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/home/bu0ai/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/home/bu0ai/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/mnt/d/docker_volume/room_of_nhanhuynh/ai_butler/CAD-ShibaSangyo/full_build_from_scratch_nhanhuynh/PaddleOCR/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=False, cls_model_dir='/home/bu0ai/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "[2024/06/28 13:17:14] ppocr WARNING: Since the angle classifier is not initialized, it will not be used during the forward process\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[[[7.0, 2.0], [46.0, 2.0], [46.0, 52.0], [7.0, 52.0]]]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PaddleOCR.paddleocr import PaddleOCR\n",
    "from PIL import ImageDraw, Image\n",
    "import os, cv2, glob\n",
    "\n",
    "from basicsr.utils import imwrite\n",
    "# Paddleocr supports Chinese, English, French, German, Korean and Japanese.\n",
    "# You can set the parameter `lang` as `ch`, `en`, `fr`, `german`, `korean`, `japan`\n",
    "# to switch the language model in order.\n",
    "ocr = PaddleOCR(ocr_version=\"PP-OCRv4\",\n",
    "                lang = 'en') # need to run only once to download and load model into memory\n",
    "\n",
    "# ocr = PaddleOCR(ocr_version=\"PP-OCR\", lang = 'en')\n",
    "\n",
    "img_path = 'temp/temp_num/0/2528_3047_2669_3266.png'\n",
    "\n",
    "img = Image.open(img_path)\n",
    "\n",
    "result = ocr.ocr(img_path, det = True,  rec=False)\n",
    "# for idx in range(len(result)):\n",
    "#     res = result[idx]\n",
    "#     for line in res:\n",
    "#         print(line)\n",
    "#         x1, y1 = line[0]\n",
    "#         x2, y2 = line[2]\n",
    "#         word = img.crop((x1, y1, x2, y2))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float('2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACkDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD1jxz4zsvA+hf2jdRPPJI4igt4zhpXxnGewwCSa5R/ib4h0KOG78U+Dp7LTJyMT20/mtACRgSAgc8+o+lZfxKA1X40eCNGnG+2X9+UJ4JLkn9IwPxr1XVdKh1fRb3T7hQ0d3C0TA/7Q/p1qkl1Jd76E9jeW+pWUF7aTrPbToHjkQ5DA15Tp8J8e/HK9vblQ2l+Fh5MCHobjP3vfkMf+ArV/wCBF3O/gS4sriTebC+mhUZ5C8Nj82aovgX/AKRoniDUn/113rEzOe/QH/2Y0mUj1auI/wCE7X+5+grt68Q/4RXX/wC7bf8Afw/4Uhom+Id6PC/xo8L+Ir5PMspbdrfpjy2ywJz7eYp/Cu38M/EOy1/U77SZ7K503U7AE3NvcYwoHUhu498Dgg96t+O/Blr438PNp80nkXMbiW1uQMmKQd/oeh/+tXkPxG8FfEC6t7fWrv7BdS2Nv9nlOnhhJPHn7zLtBYY6ryByRTvcmx2fwHtmHg2/1FlIF/qU0qN2ZBhQR+Ib8qqfCKYaH4o8W+ELhws0F61zAh43ITyR+BQ/jXU/C68in8A6Zbw6XeacLWMQtFcxlcsPvFSeoJJOaxPiT4L1SXVbXxr4TyuvWOBJCv8Ay8xjPbuwBIx3HHYUDPSrlZWtZVgIWYoQhPZscfrXzB/YHxa/556t+f8A9evbfA3xL0jxhZiOR0sNWjOyexmYKwYddueSP1Heu3zSGmf/2Q==",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=41x32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_need_up = glob.glob('temp/temp_paddle_paddle/*')\n",
    "model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)\n",
    "bg_upsampler = RealESRGANer(\n",
    "                            scale=2,\n",
    "                            model_path='https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth',\n",
    "                            model=model,\n",
    "                            tile=400,\n",
    "                            tile_pad=10,\n",
    "                            pre_pad=0,\n",
    "                            half=True)\n",
    "GFP_GAN_Model = GFPGANer(model_path='GFPGAN/experiments/pretrained_models/GFPGANv1.3.pth',\n",
    "                                upscale=2,\n",
    "                                arch='clean',\n",
    "                                channel_multiplier=2,\n",
    "                                bg_upsampler=bg_upsampler)\n",
    "def upscale_image(image_path: str, restorer,is_saved: bool = True, des_path: str = ''):\n",
    "\n",
    "\n",
    "    img_name = os.path.basename(image_path)\n",
    "    # print(f'Processing {img_name} ...')\n",
    "    basename, ext = os.path.splitext(img_name)\n",
    "    input_img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "    # restore all component if necessary\n",
    "    _, _, restored_img = restorer.enhance(\n",
    "        input_img,\n",
    "        has_aligned= False,\n",
    "        only_center_face= False,\n",
    "        paste_back= True,\n",
    "        weight= 0.5)\n",
    "    \n",
    "    if restored_img is not None:\n",
    "        if ext == 'auto':\n",
    "            extension = 'jpg'\n",
    "        else:\n",
    "            extension = ext\n",
    "        save_restore_path = os.path.join(f'{des_path}', f\"{basename}{extension}\")\n",
    "        imwrite(restored_img, save_restore_path)\n",
    "\n",
    "for image in list_need_up:\n",
    "    upscale_image(image, GFP_GAN_Model, is_saved=True, des_path='temp/temp_save_core_restore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "img = cv2.imread('temp/temp_save_core_restore/663_2424_762_2528.png', cv2.IMREAD_COLOR) \n",
    "  \n",
    "# Convert to grayscale. \n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "  \n",
    "# Blur using 3 * 3 kernel. \n",
    "gray_blurred = cv2.blur(gray, (3, 3)) \n",
    "  \n",
    "# Apply Hough transform on the blurred image. \n",
    "detected_circles = cv2.HoughCircles(gray_blurred,  \n",
    "                   cv2.HOUGH_GRADIENT, 1, 20, param1 = 50, \n",
    "               param2 = 30, minRadius = 1, maxRadius = 40) \n",
    "  \n",
    "# Draw circles that are detected. \n",
    "if detected_circles is not None: \n",
    "  \n",
    "    # Convert the circle parameters a, b and r to integers. \n",
    "    detected_circles = np.uint16(np.around(detected_circles)) \n",
    "  \n",
    "    for pt in detected_circles[0, :]: \n",
    "        a, b, r = pt[0], pt[1], pt[2] \n",
    "  \n",
    "        # Draw the circumference of the circle. \n",
    "        cv2.circle(img, (a, b), r, (0, 255, 0), 2) \n",
    "  \n",
    "        # Draw a small circle (of radius 1) to show the center. \n",
    "        cv2.circle(img, (a, b), 1, (0, 0, 255), 3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded label = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/bu0ai/.cache/torch/hub/baudm_parseq_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from strhub.data.module import SceneTextDataModule\n",
    "\n",
    "# Load model and image transforms\n",
    "parseq = torch.hub.load('baudm/parseq', 'parseq_tiny', pretrained=True).eval()\n",
    "img_transform = SceneTextDataModule.get_transform(parseq.hparams.img_size)\n",
    "\n",
    "img = Image.open('temp/temp_num/0/737_2350_825_2418.png').convert('RGB')\n",
    "# Preprocess. Model expects a batch of images with shape: (B, C, H, W)\n",
    "img = img_transform(img).unsqueeze(0)\n",
    "\n",
    "logits = parseq(img)\n",
    "logits.shape  # torch.Size([1, 26, 95]), 94 characters + [EOS] symbol\n",
    "\n",
    "# Greedy decoding\n",
    "pred = logits.softmax(-1)\n",
    "label, confidence = parseq.tokenizer.decode(pred)\n",
    "print('Decoded label = {}'.format(label[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'east_pytorch/'\n",
      "/mnt/d/docker_volume/room_of_nhanhuynh/ai_butler/CAD-ShibaSangyo/full_build_from_scratch_nhanhuynh/east_pytorch\n"
     ]
    }
   ],
   "source": [
    "%cd east_pytorch/\n",
    "!python detect.py --weights weights/model.pt --input \"/mnt/d/docker_volume/room_of_nhanhuynh/ai_butler/CAD-ShibaSangyo/full_build_from_scratch_nhanhuynh/temp/temp_char/0/592_2469_664_2562.png\" --output \"output.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shiba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
